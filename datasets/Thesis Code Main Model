#Finding Mtry an Ntree
library(caret)
set.seed(7)
shuffled_data <- data[sample(nrow(data)), ]
trainIndex <- createDataPartition(shuffled_data$total_event_count, p = 0.8, list = FALSE)
train_data <- shuffled_data[trainIndex, ]
test_data  <- shuffled_data[-trainIndex, ]

mtry_values <- 1:9
ntree_values <- c(100, 200, 500)

rf_results <- data.frame()

for (ntree_val in ntree_values) {
  tune <- expand.grid(mtry = mtry_values)
  
  set.seed(42)
  rf_model <- train(
    total_event_count ~ .,
    data = train_data,
    method = "rf",
    trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE),
    tuneGrid = tune,
    ntree = ntree_val
  )
  
  best_result <- rf_model$results[rf_model$results$mtry == rf_model$bestTune$mtry, ]
  best_result$ntree <- ntree_val
  rf_results <- rbind(rf_results, best_result)
  
  cat("Finished ntree =", ntree_val, "\n")
}

print(rf_results)

#Best optimized
set.seed(7)
shuffled_data <- data[sample(nrow(data)), ]
trainIndex <- createDataPartition(shuffled_data$total_event_count, p = 0.8, list = FALSE)
train_data <- shuffled_data[trainIndex, ]
test_data  <- shuffled_data[-trainIndex, ]

control <- trainControl(method = "cv", number = 5, verboseIter = TRUE)
tune <- expand.grid(mtry = 7)

RF_model <- train(
  total_event_count ~ .,
  data = train_data,
  method = "rf",
  trControl = control,
  tuneGrid = tune,
  ntree = 200
)

print(RF_model)


predictions <- predict(RF_model, newdata = test_data)
rmse <- sqrt(mean((test_data$total_event_count - predictions)^2))
cat("RMSE on the test data for Random Forest:", rmse, "\n")

importance_vals <- varImp(RF_model, scale = TRUE)
print(importance_vals)

plot(importance_vals, main = "Variable Importance (Random Forest)")
